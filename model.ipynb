{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0374e4f",
   "metadata": {},
   "source": [
    "MODEL 1: ReID + YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8531b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded imagenet pretrained weights from \"C:\\Users\\kaust/.cache\\torch\\checkpoints\\osnet_x1_0_imagenet.pth\"\n",
      "Successfully loaded pretrained weights from \"C:/Users/kaust/Desktop/Assignment/osnet_x1_0_imagenet.pth\"\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import torchreid\n",
    "from torchreid import models as md\n",
    "from torchreid import utils as ut\n",
    "from torchvision import transforms\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Paths to input videos\n",
    "video_path_a = \"C:/Users/kaust/Desktop/Assignment/tacticam.mp4\"\n",
    "video_path_b = \"C:/Users/kaust/Desktop/Assignment/broadcast.mp4\"\n",
    "\n",
    "# Initialize YOLO Detector\n",
    "detector = YOLO(\"best.pt\")\n",
    "\n",
    "# Initialize Re-ID model (OSNet) - pretrained on person Re-ID datasets\n",
    "reid_model = md.osnet_x1_0(num_classes=1000)\n",
    "ut.load_pretrained_weights(reid_model, 'C:/Users/kaust/Desktop/Assignment/osnet_x1_0_imagenet.pth')\n",
    "reid_model.eval().cuda()\n",
    "\n",
    "# Transformation for Re-ID input\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((256, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Video capture setup\n",
    "cap_a = cv2.VideoCapture(video_path_a)\n",
    "cap_b = cv2.VideoCapture(video_path_b)\n",
    "\n",
    "# Check and set frame properties properly\n",
    "width_a = int(cap_a.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height_a = int(cap_a.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps_a = cap_a.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "width_b = int(cap_b.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height_b = int(cap_b.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps_b = cap_b.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Ensure same resolution, or resize to match Video A\n",
    "if (width_a, height_a) != (width_b, height_b):\n",
    "    resize_video_b = True\n",
    "else:\n",
    "    resize_video_b = False\n",
    "\n",
    "# Output writer: combined width (side by side)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out_width = width_a + width_b\n",
    "out_height = max(height_a, height_b)\n",
    "out_fps = min(fps_a, fps_b) if fps_a and fps_b else 20.0\n",
    "out = cv2.VideoWriter('output_matched_video.mp4', fourcc, out_fps, (out_width, out_height))\n",
    "\n",
    "def extract_embedding(image):\n",
    "    img_tensor = transform(image).unsqueeze(0).cuda()\n",
    "    with torch.no_grad():\n",
    "        embedding = reid_model(img_tensor)\n",
    "    return embedding.cpu().numpy().flatten()\n",
    "\n",
    "def draw_boxes_with_labels(frame, detections, labels, color=(0, 255, 0)):\n",
    "    for det, label in zip(detections, labels):\n",
    "        x1, y1, x2, y2 = map(int, det)\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(frame, f\"ID: {label}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "\n",
    "while cap_a.isOpened() and cap_b.isOpened():\n",
    "    ret_a, frame_a = cap_a.read()\n",
    "    ret_b, frame_b = cap_b.read()\n",
    "\n",
    "    if not ret_a or not ret_b:\n",
    "        break\n",
    "\n",
    "    if resize_video_b:\n",
    "        frame_b = cv2.resize(frame_b, (width_a, height_a))\n",
    "\n",
    "    # YOLO detection on both frames\n",
    "    results_a = detector.predict(source=frame_a, conf=0.3, verbose=False)[0]\n",
    "    results_b = detector.predict(source=frame_b, conf=0.3, verbose=False)[0]\n",
    "\n",
    "    detections_a = results_a.boxes.xyxy.cpu().numpy()\n",
    "    detections_b = results_b.boxes.xyxy.cpu().numpy()\n",
    "\n",
    "    # Extract embeddings for detections\n",
    "    embeddings_a = []\n",
    "    for box in detections_a:\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        crop = frame_a[y1:y2, x1:x2]\n",
    "        if crop.shape[0] == 0 or crop.shape[1] == 0:\n",
    "            embeddings_a.append(np.zeros(512))  # Fallback to avoid size mismatch\n",
    "            continue\n",
    "        embeddings_a.append(extract_embedding(crop))\n",
    "\n",
    "    embeddings_b = []\n",
    "    for box in detections_b:\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        crop = frame_b[y1:y2, x1:x2]\n",
    "        if crop.shape[0] == 0 or crop.shape[1] == 0:\n",
    "            embeddings_b.append(np.zeros(512))\n",
    "            continue\n",
    "        embeddings_b.append(extract_embedding(crop))\n",
    "\n",
    "    matched_labels_a = [-1 for _ in range(len(embeddings_a))]\n",
    "    matched_labels_b = [-1 for _ in range(len(embeddings_b))]\n",
    "\n",
    "    if len(embeddings_a) > 0 and len(embeddings_b) > 0:\n",
    "        similarity_matrix = np.zeros((len(embeddings_a), len(embeddings_b)))\n",
    "        for i, emb_a in enumerate(embeddings_a):\n",
    "            for j, emb_b in enumerate(embeddings_b):\n",
    "                similarity_matrix[i, j] = 1 - cosine(emb_a, emb_b)\n",
    "        for i in range(len(embeddings_a)):\n",
    "            best_j = np.argmax(similarity_matrix[i])\n",
    "            best_i_back = np.argmax(similarity_matrix[:, best_j])\n",
    "            if best_i_back == i:\n",
    "                matched_labels_a[i] = best_j\n",
    "                matched_labels_b[best_j] = i\n",
    "\n",
    "    # Draw matched boxes\n",
    "    draw_boxes_with_labels(frame_a, detections_a, matched_labels_a, color=(0, 255, 0))\n",
    "    draw_boxes_with_labels(frame_b, detections_b, matched_labels_b, color=(0, 0, 255))\n",
    "\n",
    "    # Combine frames side by side for visualization\n",
    "    combined_frame = np.hstack((frame_a, frame_b))\n",
    "    out.write(combined_frame)\n",
    "    cv2.imshow(\"Matched Frames (Left: Video A, Right: Video B)\", combined_frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Clean up\n",
    "cap_a.release()\n",
    "cap_b.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9a8ebb",
   "metadata": {},
   "source": [
    "MODEL 2: ReID + YOLO + DeepSORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7b6f63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded imagenet pretrained weights from \"C:\\Users\\kaust/.cache\\torch\\checkpoints\\osnet_x1_0_imagenet.pth\"\n",
      "Successfully loaded pretrained weights from \"C:/Users/kaust/Desktop/Assignment/osnet_x1_0_imagenet.pth\"\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import torchreid\n",
    "from torchreid import models as md\n",
    "from torchreid import utils as ut\n",
    "from torchvision import transforms\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# ==== CONFIGURATION ====\n",
    "video_path_a = \"C:/Users/kaust/Desktop/Assignment/tacticam.mp4\"\n",
    "video_path_b = \"C:/Users/kaust/Desktop/Assignment/broadcast.mp4\"\n",
    "output_a = \"output_video_a_with_ids.mp4\"\n",
    "output_b = \"output_video_b_with_ids.mp4\"\n",
    "yolo_model_path = \"best.pt\"\n",
    "reid_weights_path = \"C:/Users/kaust/Desktop/Assignment/osnet_x1_0_imagenet.pth\"\n",
    "confidence_threshold = 0.5\n",
    "SIMILARITY_THRESHOLD = 0.6  # Adjust for tighter or looser matching\n",
    "\n",
    "# ==== INITIALIZE MODELS ====\n",
    "detector = YOLO(yolo_model_path)\n",
    "tracker_a = DeepSort(max_age=30)\n",
    "tracker_b = DeepSort(max_age=30)\n",
    "\n",
    "reid_model = md.osnet_x1_0(num_classes=1000)\n",
    "ut.load_pretrained_weights(reid_model, reid_weights_path)\n",
    "reid_model.eval().cuda()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((256, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "cap_a = cv2.VideoCapture(video_path_a)\n",
    "cap_b = cv2.VideoCapture(video_path_b)\n",
    "\n",
    "width_a, height_a, fps_a = int(cap_a.get(3)), int(cap_a.get(4)), cap_a.get(cv2.CAP_PROP_FPS) or 20.0\n",
    "width_b, height_b, fps_b = int(cap_b.get(3)), int(cap_b.get(4)), cap_b.get(cv2.CAP_PROP_FPS) or 20.0\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out_a = cv2.VideoWriter(output_a, fourcc, fps_a, (width_a, height_a))\n",
    "out_b = cv2.VideoWriter(output_b, fourcc, fps_b, (width_b, height_b))\n",
    "\n",
    "while cap_a.isOpened() and cap_b.isOpened():\n",
    "    ret_a, frame_a = cap_a.read()\n",
    "    ret_b, frame_b = cap_b.read()\n",
    "    if not ret_a or not ret_b: break\n",
    "\n",
    "    h_a, w_a, _ = frame_a.shape\n",
    "    h_b, w_b, _ = frame_b.shape\n",
    "\n",
    "    # ==== DETECTION ====\n",
    "    preds_a = detector.predict(frame_a, conf=confidence_threshold, verbose=False)[0]\n",
    "    preds_b = detector.predict(frame_b, conf=confidence_threshold, verbose=False)[0]\n",
    "\n",
    "    detections_a = preds_a.boxes.xyxy.cpu().numpy() if preds_a.boxes.xyxy is not None else []\n",
    "    detections_b = preds_b.boxes.xyxy.cpu().numpy() if preds_b.boxes.xyxy is not None else []\n",
    "    confs_a = preds_a.boxes.conf.cpu().numpy() if preds_a.boxes.conf is not None else np.ones(len(detections_a))\n",
    "    confs_b = preds_b.boxes.conf.cpu().numpy() if preds_b.boxes.conf is not None else np.ones(len(detections_b))\n",
    "\n",
    "    dets_for_tracker_a = [[[float(x1), float(y1), float(x2), float(y2)], float(c), 0] for (x1, y1, x2, y2), c in zip(detections_a, confs_a)]\n",
    "    dets_for_tracker_b = [[[float(x1), float(y1), float(x2), float(y2)], float(c), 0] for (x1, y1, x2, y2), c in zip(detections_b, confs_b)]\n",
    "\n",
    "    tracks_a = tracker_a.update_tracks(dets_for_tracker_a, frame=frame_a)\n",
    "    tracks_b = tracker_b.update_tracks(dets_for_tracker_b, frame=frame_b)\n",
    "\n",
    "    # ==== EXTRACT EMBEDDINGS ====\n",
    "    emb_a, ids_a, boxes_a = [], [], []\n",
    "    for track in tracks_a:\n",
    "        if not track.is_confirmed(): continue\n",
    "        x1, y1, x2, y2 = map(int, track.to_ltrb())\n",
    "        # Clip bounding boxes\n",
    "        x1, y1, x2, y2 = max(0, x1), max(0, y1), min(w_a - 1, x2), min(h_a - 1, y2)\n",
    "        crop = frame_a[y1:y2, x1:x2]\n",
    "        if crop.shape[0] == 0 or crop.shape[1] == 0: continue\n",
    "        img = transform(crop).unsqueeze(0).cuda()\n",
    "        with torch.no_grad(): emb = reid_model(img)\n",
    "        emb_a.append(emb.cpu().numpy().flatten())\n",
    "        ids_a.append(track.track_id)\n",
    "        boxes_a.append((x1, y1, x2, y2))\n",
    "\n",
    "    emb_b, ids_b, boxes_b = [], [], []\n",
    "    for track in tracks_b:\n",
    "        if not track.is_confirmed(): continue\n",
    "        x1, y1, x2, y2 = map(int, track.to_ltrb())\n",
    "        # Clip bounding boxes\n",
    "        x1, y1, x2, y2 = max(0, x1), max(0, y1), min(w_b - 1, x2), min(h_b - 1, y2)\n",
    "        crop = frame_b[y1:y2, x1:x2]\n",
    "        if crop.shape[0] == 0 or crop.shape[1] == 0: continue\n",
    "        img = transform(crop).unsqueeze(0).cuda()\n",
    "        with torch.no_grad(): emb = reid_model(img)\n",
    "        emb_b.append(emb.cpu().numpy().flatten())\n",
    "        ids_b.append(track.track_id)\n",
    "        boxes_b.append((x1, y1, x2, y2))\n",
    "\n",
    "    # ==== CROSS-VIDEO MATCHING with SIMILARITY THRESHOLD ====\n",
    "    match_labels_a, match_labels_b = [-1]*len(emb_a), [-1]*len(emb_b)\n",
    "    if len(emb_a) > 0 and len(emb_b) > 0:\n",
    "        sim_matrix = np.array([[1 - cosine(ea, eb) for eb in emb_b] for ea in emb_a])\n",
    "        for i in range(len(emb_a)):\n",
    "            best_j = np.argmax(sim_matrix[i])\n",
    "            if np.argmax(sim_matrix[:, best_j]) == i and sim_matrix[i, best_j] > SIMILARITY_THRESHOLD:\n",
    "                match_labels_a[i] = ids_b[best_j]\n",
    "                match_labels_b[best_j] = ids_a[i]\n",
    "\n",
    "    # ==== DRAW RESULTS ====\n",
    "    for (x1, y1, x2, y2), tid, mid in zip(boxes_a, ids_a, match_labels_a):\n",
    "        cv2.rectangle(frame_a, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(frame_a, f\"T:{tid} M:{mid}\" if mid != -1 else f\"T:{tid}\", (x1, y1-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "    for (x1, y1, x2, y2), tid, mid in zip(boxes_b, ids_b, match_labels_b):\n",
    "        cv2.rectangle(frame_b, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "        cv2.putText(frame_b, f\"T:{tid} M:{mid}\" if mid != -1 else f\"T:{tid}\", (x1, y1-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "    out_a.write(frame_a)\n",
    "    out_b.write(frame_b)\n",
    "    cv2.imshow(\"Video A\", frame_a)\n",
    "    cv2.imshow(\"Video B\", frame_b)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): break\n",
    "\n",
    "cap_a.release(); cap_b.release(); out_a.release(); out_b.release(); cv2.destroyAllWindows()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
